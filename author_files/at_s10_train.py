# -*- coding: utf-8 -*-
"""at_s10_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ay7HqOSSJLgiE1yTuprxiFlcqaKOpJ_
"""

# Commented out IPython magic to ensure Python compatibility.
# # code based on https://gofilipa.github.io/664/analyzing/generating/fine-tune.html
# 
# %%capture
# %pip install transformers trl

from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    pipeline
)
from trl import SFTTrainer, SFTConfig

from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

ds = load_dataset('csv', data_files='/content/drive/MyDrive/adventuretime/at_s10_text.csv')
ds

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

training_params = SFTConfig(
    output_dir='/content/drive/MyDrive/adventuretime',
    num_train_epochs = 3, # how many times we iterate over the dataset as a whole
    learning_rate = 2e-4, # how many "steps" we take in adjusting the parameters to make up for loss
    weight_decay = 0.001, # way of regularizing the parameters
    dataset_text_field = "text",
    report_to="none", # this is a new param, to avoid a login to W&B
    fp16=True,
    label_names = 'text',
)

trainer = SFTTrainer(
    model = model,
    train_dataset = ds['train'],
    processing_class = tokenizer,
    args = training_params,

)

trainer.train()

## first, save the model to a folder called "models"
trainer.model.save_pretrained("models")
trainer.tokenizer.save_pretrained("models")

## then, load our model from that folder
model = AutoModelForCausalLM.from_pretrained("./models")
tokenizer = AutoTokenizer.from_pretrained("./models")

## create a pipe() function that calls our new model
pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=50)

### run inference!
pipe("I love")

